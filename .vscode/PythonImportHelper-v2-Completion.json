[
    {
        "label": "runpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "runpy",
        "description": "runpy",
        "detail": "runpy",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "Agent",
        "importPath": "agno.agent",
        "description": "agno.agent",
        "isExtraImport": true,
        "detail": "agno.agent",
        "documentation": {}
    },
    {
        "label": "OpenAIChat",
        "importPath": "agno.models.openai",
        "description": "agno.models.openai",
        "isExtraImport": true,
        "detail": "agno.models.openai",
        "documentation": {}
    },
    {
        "label": "WebCrawlerTool",
        "importPath": "web_crawler_tool",
        "description": "web_crawler_tool",
        "isExtraImport": true,
        "detail": "web_crawler_tool",
        "documentation": {}
    },
    {
        "label": "WebCrawlerTool",
        "importPath": "web_crawler_tool",
        "description": "web_crawler_tool",
        "isExtraImport": true,
        "detail": "web_crawler_tool",
        "documentation": {}
    },
    {
        "label": "WebCrawlerTool",
        "importPath": "web_crawler_tool",
        "description": "web_crawler_tool",
        "isExtraImport": true,
        "detail": "web_crawler_tool",
        "documentation": {}
    },
    {
        "label": "ReasoningTools",
        "importPath": "agno.tools.reasoning",
        "description": "agno.tools.reasoning",
        "isExtraImport": true,
        "detail": "agno.tools.reasoning",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "xml.etree.ElementTree",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xml.etree.ElementTree",
        "description": "xml.etree.ElementTree",
        "detail": "xml.etree.ElementTree",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "urljoin",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "Toolkit",
        "importPath": "agno.tools",
        "description": "agno.tools",
        "isExtraImport": true,
        "detail": "agno.tools",
        "documentation": {}
    },
    {
        "label": "AsyncWebCrawler",
        "importPath": "crawl4ai",
        "description": "crawl4ai",
        "isExtraImport": true,
        "detail": "crawl4ai",
        "documentation": {}
    },
    {
        "label": "CrawlerRunConfig",
        "importPath": "crawl4ai",
        "description": "crawl4ai",
        "isExtraImport": true,
        "detail": "crawl4ai",
        "documentation": {}
    },
    {
        "label": "aiohttp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiohttp",
        "description": "aiohttp",
        "detail": "aiohttp",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"bin\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"ask-ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.13/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"bin\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"ask-ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.13/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"ask-ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.13/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"ask-ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.13/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = \"ask-ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.13/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"../lib/python3.13/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": ".venv.bin.activate_this",
        "description": ".venv.bin.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": ".venv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "create_web_support_agent",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def create_web_support_agent(starting_urls: list):\n    \"\"\"Create a web support agent with crawling capabilities.\"\"\"\n    # Create the crawler tool - it will automatically extract allowed domains from starting URLs\n    crawler_tool = WebCrawlerTool(starting_urls=starting_urls)\n    # Create agent with intelligent instructions\n    agent = Agent(\n        model=OpenAIChat(id=\"gpt-4.1\", api_key=os.getenv(\"OPENAI_API_KEY\")),\n        tools=[crawler_tool,ReasoningTools()],\n        description=f\"You are an agent that answers user queries based exclusively on content from the starting URLs: {', '.join(starting_urls)}. The starting URLs serve only as the content source - you crawl them to get information and answer questions based on that content.\",\n        instructions=[",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "demo_web_support_bot",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def demo_web_support_bot():\n    \"\"\"Demo the web support bot with example websites.\"\"\"\n    print(\"=== Web Support Bot Demo ===\\n\")\n    # Example: Create agent for a hypothetical e-commerce site\n    starting_urls = [\"https://docs.agno.com/introduction\"]\n    agent = create_web_support_agent(starting_urls)\n    # Get the automatically extracted domains for display\n    allowed_domains = agent.tools[0].allowed_domains\n    print(\"ðŸ¤– Web Support Agent created!\")\n    print(f\"ðŸ“ Starting URLs: {', '.join(starting_urls)}\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "interactive_mode",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def interactive_mode():\n    \"\"\"Interactive mode where user can specify their own URLs and ask questions.\"\"\"\n    print(\"=== Interactive Web Support Bot ===\\n\")\n    # Get starting URLs from user\n    print(\"Enter starting URLs (comma-separated):\")\n    urls_input = input(\"> \")\n    starting_urls = [url.strip() for url in urls_input.split(\",\") if url.strip()]\n    if not starting_urls:\n        print(\"âŒ No valid URLs provided. Using example.com as default.\")\n        starting_urls = [\"https://example.com\"]",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "WebCrawlerTool",
        "kind": 6,
        "importPath": "web_crawler_tool",
        "description": "web_crawler_tool",
        "peekOfCode": "class WebCrawlerTool(Toolkit):\n    \"\"\"Simple web crawler tool that returns content and links for agent decision making.\"\"\"\n    def __init__(self, starting_urls: List[str] = None, max_links_per_page: int = 50):\n        super().__init__()\n        self.starting_urls = starting_urls or []\n        self.allowed_domains = self._extract_domains_from_urls(self.starting_urls)\n        self.max_links_per_page = max_links_per_page\n        self.register(self.crawl_selected_urls)\n    def _extract_domains_from_urls(self, urls: List[str]) -> List[str]:\n        \"\"\"Extract unique domains from a list of URLs.\"\"\"",
        "detail": "web_crawler_tool",
        "documentation": {}
    }
]